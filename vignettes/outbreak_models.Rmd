---
title: "Outbreak modelling"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Outbreak modelling}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>")
```

This vignette demonstrates the minimal outbreak-modelling workflow on the shipped
example dataset.

The key steps are:

1. Create a modelling-ready dataset using [make_modelling_panel()].
2. Fit a simple baseline model with [fit_outbreak_models()].
3. Evaluate predictive performance with [evaluate_models()].

All examples here run without network access.

## Build the modelling panel

```{r}
library(vpdsus)

panel <- vpdsus_example_panel()

# Susceptibility method A (static): uses a population column and a coverage column
suscept <- estimate_susceptible_static(panel, coverage_col = "coverage", pop_col = "pop_0_4")

# Create a modelling dataset with a 1-year lag between predictors and outcome.
# outcome='outbreak_pc' uses a simple per-100k threshold defined in
# make_outcome_outbreak().
mod <- make_modelling_panel(panel, suscept, outcome = "outbreak_pc", lag_years = 1)
mod
```

Columns in `mod`:

- `susceptible_prop`: predictor (fraction susceptible)
- `outcome`: binary outbreak indicator in `year_next`
- `cases_per_100k` is available from [make_outcome_outbreak()] on the original panel

## Fit a baseline outbreak model

`fit_outbreak_models()` currently fits a simple logistic regression:

- outcome ~ susceptible_prop + year + who_region

```{r}
fit <- fit_outbreak_models(mod)
fit$model
```

### Coefficients

```{r}
fit$coefficients
```

## Simple time-aware evaluation

`evaluate_models()` performs a time split by year (train on earlier years, test on
later years) and reports a basic accuracy and Brier score.

By default, the split chooses a `train_end` year that guarantees at least one
distinct year is held out for testing.

```{r}
ev <- evaluate_models(mod)
ev
```

Interpretation:

- `train_end`: last year included in training
- `n_train` / `n_test`: number of rows used for training/testing
- `accuracy`: fraction of correctly classified outcomes at a 0.5 probability threshold
- `brier`: mean squared error of predicted probabilities (lower is better; 0 is perfect)

### Inspect predicted probabilities

If you want to see the actual predicted probabilities used in the evaluation,
repeat the same split and call `predict()` on the fitted model:

```{r}
# Use the same train_end chosen by evaluate_models()
train_end <- ev$train_end

train <- subset(mod, year <= train_end)
test <- subset(mod, year > train_end)

fit2 <- fit_outbreak_models(train)$model
p <- predict(fit2, newdata = test, type = "response")

out <- transform(test,
  prob_outbreak = p,
  pred_outbreak = as.integer(p >= 0.5)
)

out[, c("iso3", "year", "year_next", "outcome", "prob_outbreak", "pred_outbreak")]
```
